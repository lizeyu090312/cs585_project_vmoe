{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmFWTnx_PQ-"
      },
      "source": [
        "## Evaluate E$^3$-S/32, with 8 experts, pre-trained on ILSVRC2021 and fine-tuned on CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8lQf6R1BOYlp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-09 17:42:11.518831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/zl310/anaconda3/envs/cs585_tf_google/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "from jax import numpy as jnp\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from vmoe.nn import models\n",
        "from vmoe.data import input_pipeline\n",
        "from vmoe.checkpoints import partitioned\n",
        "\n",
        "from vmoe.configs.vmoe_paper.vmoe_s32_last2_ilsvrc2012_randaug_light1_ft_ilsvrc2012 import get_config, IMAGE_SIZE, BATCH_SIZE\n",
        "\n",
        "import os\n",
        "# change configuration in the above file.\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "_ = \"\"\"\n",
        "Adapted from vmoe/notebooks/demo_eee_CIFAR100.ipynb by Michael Li\n",
        "Structure:\n",
        "vmoe\n",
        "    vmoe/\n",
        "    this notebook\n",
        "    vit_jax/ (from vision_transformer)\n",
        "    vmoe_s32_last2_ilsvrc2012_randaug_light1_ft_ilsvrc2012.data-00000-of-00001\n",
        "    vmoe_s32_last2_ilsvrc2012_randaug_light1_ft_ilsvrc2012.index\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_6jHA1f_ef3"
      },
      "source": [
        "### Construct model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J2MwqMchM1yY"
      },
      "outputs": [],
      "source": [
        "model_config = get_config()\n",
        "# print(model_config)\n",
        "model_cls = getattr(models, model_config.model.name)\n",
        "model = model_cls(deterministic=True, **model_config.model)\n",
        "# print(type(model))\n",
        "# print(model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djlprye0_oOd"
      },
      "source": [
        "### Load weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wcDwZbA3coMY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Encoder', 'cls', 'embedding', 'head'])\n"
          ]
        }
      ],
      "source": [
        "# using this model: 'gs://vmoe_checkpoints/vmoe_s32_last2_ilsvrc2012_randaug_light1_ft_ilsvrc2012'\n",
        "checkpoint_prefix = 'vmoe_s32_last2_ilsvrc2012_randaug_light1_ft_ilsvrc2012'\n",
        "checkpoint = partitioned.restore_checkpoint(prefix=checkpoint_prefix, tree=None)\n",
        "\n",
        "print(checkpoint.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2yxvTOt_r-8"
      },
      "source": [
        "### Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yt4gIiYRTFog"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-09 17:42:14.062093: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self.hidden_size 512, self.patch_size (32, 32), self.patch_size (32, 32)\n",
            "VisionTransformerMoe.__call__, x.shape (32, 144, 512)\n",
            "x.shape (32, 144, 512) before jnp.concatenate([cls, x], axis=1)\n",
            "self.param('cls', nn.initializers.zeros, (1, 1, x.shape[-1])), cls.shape (1, 1, 512)\n",
            "jnp.tile(cls, [x.shape[0], 1, 1]) has shape (32, 1, 512)\n",
            "x.shape (32, 145, 512) immediately before self.encoder_cls\n",
            "in EncoderMoe, inputs.shape (32, 145, 512)\n",
            "in EncoderMoe, x.shape (32, 145, 512)\n",
            "block in EncoderMoe 0, x.shape = (32, 145, 512)\n",
            "block in EncoderMoe 1, x.shape = (32, 145, 512)\n",
            "block in EncoderMoe 2, x.shape = (32, 145, 512)\n",
            "block in EncoderMoe 3, x.shape = (32, 145, 512)\n",
            "block in EncoderMoe 4, x.shape = (32, 145, 512)\n",
            "block in EncoderMoe 5, x.shape = (32, 145, 512)\n",
            "gates.shape (1160, 8)\n",
            "buffer_idx.shape (1160, 8)\n",
            "dispatch_weights.shape (1160, 8, 436)\n",
            "indices.shape (4, 1160, 512), inputs[0].shape (4, 1160, 512) before dispatch in transformed\n",
            "self.dispatch_weights.shape (4, 1160, 8, 436) in EinsumDispatcher.dispatch()\n",
            "data.shape in EinsumDispatcher's dispatch (4, 1160, 512) before jnp.einsum\n",
            "self.dispatch_weights.shape (4, 1160, 8, 436) in EinsumDispatcher.dispatch()\n",
            "data.shape in EinsumDispatcher's dispatch (4, 1160, 512) before jnp.einsum\n",
            "indices_out.shape (8, 1744, 512), inputs[0].shape (8, 1744, 512) after dispatch in transformed\n",
            "from orig inputs [-1.3174773  -0.7806566   4.3464394   4.909447    0.8724266   0.01120377\n",
            " -1.505288   -0.7036985  -0.30758905  0.0166415   0.86710644  0.47167423\n",
            " -0.4899367   2.360216   -3.9545364 ]\n",
            "from dispatched inputs [-1.3203125  -0.78125     4.34375     4.90625     0.87109375  0.01123047\n",
            " -1.5078125  -0.703125   -0.30664062  0.01660156  0.8671875   0.47070312\n",
            " -0.49023438  2.359375   -3.953125  ]\n",
            "subtraction of the two [ 2.8351545e-03  5.9342384e-04  2.6893616e-03  3.1971931e-03\n",
            "  1.3328791e-03 -2.6702881e-05  2.5244951e-03 -5.7351589e-04\n",
            " -9.4842911e-04  3.9935112e-05 -8.1062317e-05  9.7110868e-04\n",
            "  2.9766560e-04  8.4090233e-04 -1.4114380e-03]\n",
            "outputs.shape (8, 1744, 512) before jax.tree_util.tree_map(dispatcher.combine, outputs)\n",
            "_receive data.shape (4, 8, 436, 512)\n",
            "sparse_moe_spmd outputs.shape (4, 1160, 512)\n",
            "block in EncoderMoe 6, x.shape = (32, 145, 512)\n",
            "block in EncoderMoe 7, x.shape = (32, 145, 512)\n",
            "gates.shape (1160, 8)\n",
            "buffer_idx.shape (1160, 8)\n",
            "dispatch_weights.shape (1160, 8, 436)\n",
            "indices.shape (4, 1160, 512), inputs[0].shape (4, 1160, 512) before dispatch in transformed\n",
            "self.dispatch_weights.shape (4, 1160, 8, 436) in EinsumDispatcher.dispatch()\n",
            "data.shape in EinsumDispatcher's dispatch (4, 1160, 512) before jnp.einsum\n",
            "self.dispatch_weights.shape (4, 1160, 8, 436) in EinsumDispatcher.dispatch()\n",
            "data.shape in EinsumDispatcher's dispatch (4, 1160, 512) before jnp.einsum\n",
            "indices_out.shape (8, 1744, 512), inputs[0].shape (8, 1744, 512) after dispatch in transformed\n",
            "from orig inputs [ 4.417201   -1.6042259  -1.3831747   1.1232679   1.1330428   0.9636196\n",
            " -0.3182069  -0.34469128 -5.012992    0.01186663  0.1917125  -0.90949076\n",
            " -0.09924182  1.6479409  -4.085187  ]\n",
            "from dispatched inputs [ 4.40625    -1.6015625  -1.3828125   1.125       1.1328125   0.96484375\n",
            " -0.31835938 -0.34375    -5.          0.01184082  0.19140625 -0.91015625\n",
            " -0.09912109  1.6484375  -4.09375   ]\n",
            "subtraction of the two [ 1.0951042e-02 -2.6633739e-03 -3.6215782e-04 -1.7321110e-03\n",
            "  2.3031235e-04 -1.2241602e-03  1.5246868e-04 -9.4127655e-04\n",
            " -1.2991905e-02  2.5808811e-05  3.0624866e-04  6.6548586e-04\n",
            " -1.2072921e-04 -4.9662590e-04  8.5630417e-03]\n",
            "outputs.shape (8, 1744, 512) before jax.tree_util.tree_map(dispatcher.combine, outputs)\n",
            "_receive data.shape (4, 8, 436, 512)\n",
            "sparse_moe_spmd outputs.shape (4, 1160, 512)\n",
            "encoded.shape (32, 145, 512)\n",
            "x.shape (32, 145, 512) after self.encoder_cls()\n",
            "self.classifier = token, x.shape=(32, 512)\n",
            "logits.shape (32, 1000) at return\n",
            "Test accuracy, iteration: 87.50%\n"
          ]
        }
      ],
      "source": [
        "dataset_config_test = model_config.dataset.test\n",
        "dataset_test = input_pipeline.get_dataset(\n",
        "    variant='test',\n",
        "    name=dataset_config_test.name, \n",
        "    split=dataset_config_test.split, \n",
        "    batch_size=dataset_config_test.batch_size, \n",
        "    process=dataset_config_test.process\n",
        ")\n",
        "\n",
        "# dataset_config_test_real = model_config.dataset.test_real\n",
        "# dataset_test_real = input_pipeline.get_dataset(\n",
        "#     variant='test',\n",
        "#     name=dataset_config_test_real.name, \n",
        "#     split=dataset_config_test_real.split, \n",
        "#     batch_size=dataset_config_test_real.batch_size, \n",
        "#     process=dataset_config_test_real.process\n",
        "# )\n",
        "\n",
        "def process_indices(indices_distr, class_lbl, fdir, fname):\n",
        "    # indices_distr has shape (8, 55808, 512) for batch_size = 1024\n",
        "    return\n",
        "\n",
        "\n",
        "def gen_data(model, dataset, checkpoint):\n",
        "    ncorrect = 0\n",
        "    ntotal = 0\n",
        "    i = 0\n",
        "\n",
        "    for batch in dataset:\n",
        "        # The final batch has been padded with fake examples so that the batch size is\n",
        "        # the same as all other batches. The mask tells us which examples are fake.\n",
        "        mask = batch['__valid__']\n",
        "        if jnp.sum(mask) != BATCH_SIZE:  # if there are some padded fake data inside of the current batch\n",
        "            break\n",
        "        # print(mask.shape)  # array of shape batch_size with boolean\n",
        "        logits, _, indices_distr = model.apply({'params': checkpoint}, batch['image'])\n",
        "    \n",
        "        log_p = jax.nn.log_softmax(logits)\n",
        "        preds = jnp.argmax(log_p, axis=1)\n",
        "        true_lbl = jnp.argmax(batch['labels'], axis=1)\n",
        "        process_indices(indices_distr, class_lbl=true_lbl, fdir='', fname='')\n",
        "\n",
        "        ncorrect += jnp.sum((preds == true_lbl) * mask)\n",
        "        ntotal += jnp.sum(mask)\n",
        "        # if i % 10 == 0:\n",
        "        #   print(f'Test accuracy, iteration {i}: {ncorrect / ntotal * 100:.2f}%')\n",
        "        i += 1\n",
        "        break\n",
        "    print(f'Test accuracy, iteration: {ncorrect / ntotal * 100:.2f}%')\n",
        "    return indices_distr\n",
        "\n",
        "ind_dist = gen_data(model, dataset_test, checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i, j, k = ind_dist['idx_5'].shape\n",
        "# dd = {}\n",
        "# for ii in range(i):\n",
        "#     for jj in range(j):\n",
        "#         if int(ind_dist['idx_5'][ii, jj, 0]) != 0:\n",
        "#             mul = int(ind_dist['idx_5'][ii, jj, 0] - 1) * 145 + int(ind_dist['idx_5'][ii, jj, 1] - 1)\n",
        "#             if mul not in dd.keys():\n",
        "#                 dd[mul] = 1\n",
        "#             else:\n",
        "#                 dd[mul] += 1\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# # for key in dd.keys():\n",
        "# #     if dd[key] == 66:\n",
        "# #         print(key)\n",
        "# # print(max(dd.values()))\n",
        "# plt.bar(np.array(list(dd.keys())), np.array(list(dd.values())))\n",
        "# plt.xlabel(\"patch index\")\n",
        "# plt.ylabel(\"number of times the patch is assigned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(np.average(list(dd.values())))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "provenance": [
        {
          "file_id": "1etGGfOO1WjJmEDyWqdNz5XgSMsVb5Z_d",
          "timestamp": 1662960558723
        }
      ]
    },
    "kernelspec": {
      "display_name": "cs585_tf_google",
      "language": "python",
      "name": "cs585_tf_google"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
